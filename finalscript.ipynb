{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load mytest.py\n",
    "from model import DeepHomographyModel\n",
    "import torch\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "net = DeepHomographyModel()\n",
    "\n",
    "net.load_state_dict(torch.load('model_final_Vanilla'))\n",
    "\n",
    "img = cv2.imread('handwriting1.tif', 0)\n",
    "img = cv2.resize(img, (320, 240))\n",
    "\n",
    "test_image = img.copy()\n",
    "\n",
    "top_x = random.randint(0+32, 320-32-128)   #The structure of this part of the code is cited from  https://github.com/paeccher/Deep-Homography-Estimation-Pytorch/blob/master/dataset.py\n",
    "top_y = random.randint(0+32, 240-32-128)\n",
    "\n",
    "top_left_point = (top_x, top_y)\n",
    "top_right_point = (top_x+128, top_y)\n",
    "bottom_left_point = (top_x, top_y+128)\n",
    "\n",
    "bottom_right_point = (top_x+128, top_y+128)\n",
    "\n",
    "four_points = [top_left_point, top_right_point, bottom_left_point, bottom_right_point]\n",
    "\n",
    "perturbed_four_points = []\n",
    "for point in four_points:\n",
    "    perturbed_four_points.append( (point[0] + random.randint(-32,32), point[1] + random.randint(-32,32)) )\n",
    "\n",
    "H = cv2.getPerspectiveTransform( np.float32(four_points), np.float32(perturbed_four_points) )\n",
    "\n",
    "H_inverse = inv(H)\n",
    "\n",
    "warped_image = cv2.warpPerspective(img, H_inverse, (320,240))   #The citation ends here, note that only the structure was cited, the parameters are self-designed\n",
    "\n",
    "Ip1 = test_image[top_left_point[1]:bottom_right_point[1],top_left_point[0]:bottom_right_point[0]]\n",
    "Ip2 = warped_image[top_left_point[1]:bottom_right_point[1],top_left_point[0]:bottom_right_point[0]]\n",
    "training_image = np.dstack((Ip1, Ip2))\n",
    "H_four_points = np.subtract(np.array(perturbed_four_points), np.array(four_points))\n",
    "\n",
    "datum = (training_image, H_four_points)\n",
    "tensortraining_image = torch.from_numpy(training_image)\n",
    "tensortraining_image = tensortraining_image.unsqueeze(0)\n",
    "tensortraining_image = tensortraining_image.permute(0,3,1,2).float()\n",
    "\n",
    "outputs = net.forward(tensortraining_image)\n",
    "outputs = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Image',img) #This part shows the original sample image\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('WarpedImage',warped_image) #This part shows the warped image that is also used as the input of the network\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "padimg = cv2.copyMakeBorder( img, 75, 75, 75, 75,cv2.BORDER_CONSTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('padImage',padimg) #This part shows the padded original image\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "padshiyan=cv2.warpPerspective(padimg,H_inverse,(320+500,240+500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d62a28a2964a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Image'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadshiyan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "cv2.imshow('Image',padshiyan) #This part shows the warped padimg, the transformation matrix is exactly the same as the one above\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 10.2902,  16.3866, -30.2348,   8.1603, -36.9843,  42.8587,  22.2921,\n",
       "          13.1488]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs #This part shows the output of the network, the transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "npoutputs=outputs.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10.290247,  16.38657 , -30.234777,   8.160304, -36.98431 ,\n",
       "         42.85875 ,  22.292082,  13.148786]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npoutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=np.matrix([(0,0),(320-1,0),(0,240-1),(320-1,240-1)])\n",
    "a2=np.matrix([(0+npoutputs[0,0],0+npoutputs[0,1]),(320-1+npoutputs[0,2],0+npoutputs[0,3]),(0+npoutputs[0,4],240-1+npoutputs[0,5]),(320-1+npoutputs[0,6],240-1+npoutputs[0,7])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformationmatrix=cv2.getPerspectiveTransform( np.float32(a1), np.float32(a2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.22745789e-01, -1.55792789e-01,  1.02902470e+01],\n",
       "       [-2.43808214e-02,  7.90612467e-01,  1.63865700e+01],\n",
       "       [ 1.72401174e-04, -1.13585209e-03,  1.00000000e+00]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformationmatrix #This part shows the equivalent 3x3 tranformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Itransformationmatrix=inv(transformationmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09155173e+00,  1.94380523e-01, -1.44175669e+01],\n",
       "       [ 3.66976111e-02,  1.24228498e+00, -2.07344172e+01],\n",
       "       [-1.46501742e-04,  1.37754055e-03,  9.78934374e-01]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Itransformationmatrix #This part shows the inverse of the transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "padshiyan1=cv2.warpPerspective(padimg,transformationmatrix,(320+500,240+500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('padshiyan1',padshiyan1)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "padshiyan2=cv2.warpPerspective(padimg,Itransformationmatrix,(320+500,240+500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Image',padshiyan2)#We try to map the warped image to the original image. This part shows the result.\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore1=cv2.warpPerspective(padshiyan,transformationmatrix,(320+500,240+500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('restore1',restore1)#We try to map the original image to the warped image. This part shows the result.\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
